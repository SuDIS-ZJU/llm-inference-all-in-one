# ğŸš€ **LLM Inference All-in-One** ğŸŒŸ

Your ultimate guide to resources, papers, and blogs on **Large Language Model (LLM)** inference techniques! ğŸ“šâœ¨

---

## ğŸ† **Awesome Lists**

### **Overview**  
ğŸ”— [**Awesome-LLM-Inference**](https://github.com/DefTruth/Awesome-LLM-Inference)  
A curated collection of papers and codes on LLM inference, including topics like FlashAttention, PagedAttention, and Parallelism.

---

### ğŸŒ€ **Speculative Decoding**
Explore advanced methods for accelerating LLM decoding with speculative techniques. ğŸš€

---

### ğŸ“ **Long-Context Modeling**  
ğŸ”— [**Large Language Model Based Long Context Modeling Papers and Blogs**](https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling)  
Dive deep into papers and blogs on extending LLM context length, efficient transformers, and retrieval-augmented generation (RAG). ğŸ§ âœ¨

---

### ğŸ§© **Mixture of Experts (MoE)**  
ğŸ”— [**Awesome MoE LLM Inference System and Algorithm**](https://github.com/MoE-Inf/awesome-moe-inference/)  
A comprehensive list of resources for optimizing MoE-based LLM inference. Perfect for tackling sparse expert models! ğŸŒŸ

---

### ğŸ—‚ï¸ **KV Cache Management**  
Efficient management of KV Caches for LLM acceleration! âš¡

- ğŸ”— [**Awesome-KV-Cache-Management**](https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management)  
  Explore token-level, model-level, and system-level optimizations for KV Cache.  
- ğŸ”— [**Awesome-KV-Cache-Compression**](https://github.com/October2001/Awesome-KV-Cache-Compression)  
  Must-read papers on KV Cache compression for memory-efficient LLM inference.

---

## ğŸ“ **Blogs**

Explore insightful blogs and write-ups on cutting-edge LLM inference techniques! ğŸŒ  
Stay tuned for more updates! ğŸ‰
