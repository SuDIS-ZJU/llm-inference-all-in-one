# 🚀 **LLM Inference All-in-One** 🌟

Your ultimate guide to resources, papers, and blogs on **Large Language Model (LLM)** inference techniques! 📚✨

---

## 🏆 **Awesome Lists**

### **Overview**  
🔗 [**Awesome-LLM-Inference**](https://github.com/DefTruth/Awesome-LLM-Inference)  
A curated collection of papers and codes on LLM inference, including topics like FlashAttention, PagedAttention, and Parallelism.

---

### 🌀 **Speculative Decoding**
Explore advanced methods for accelerating LLM decoding with speculative techniques. 🚀

---

### 📏 **Long-Context Modeling**  
🔗 [**Large Language Model Based Long Context Modeling Papers and Blogs**](https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling)  
Dive deep into papers and blogs on extending LLM context length, efficient transformers, and retrieval-augmented generation (RAG). 🧠✨

---

### 🧩 **Mixture of Experts (MoE)**  
🔗 [**Awesome MoE LLM Inference System and Algorithm**](https://github.com/MoE-Inf/awesome-moe-inference/)  
A comprehensive list of resources for optimizing MoE-based LLM inference. Perfect for tackling sparse expert models! 🌟

---

### 🗂️ **KV Cache Management**  
Efficient management of KV Caches for LLM acceleration! ⚡

- 🔗 [**Awesome-KV-Cache-Management**](https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management)  
  Explore token-level, model-level, and system-level optimizations for KV Cache.  
- 🔗 [**Awesome-KV-Cache-Compression**](https://github.com/October2001/Awesome-KV-Cache-Compression)  
  Must-read papers on KV Cache compression for memory-efficient LLM inference.

---

## 📝 **Blogs**

Explore insightful blogs and write-ups on cutting-edge LLM inference techniques! 🌐  
Stay tuned for more updates! 🎉
